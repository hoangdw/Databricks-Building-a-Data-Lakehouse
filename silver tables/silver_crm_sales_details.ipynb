{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08c81881-b3c2-4c45-85f1-cf26f4a1c7e7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reusable Data Cleaning Functions for Sales Details"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from typing import Dict\n",
    "from itertools import chain\n",
    "\n",
    "def create_map_from_dict(mapping: Dict[str, str]):\n",
    "    return F.create_map([F.lit(x) for x in chain(*mapping.items())])\n",
    "\n",
    "def trim_all_string_columns(df: DataFrame) -> DataFrame:\n",
    "    str_cols = {f.name for f in df.schema.fields if isinstance(f.dataType, StringType)}\n",
    "    return df.select(\n",
    "        *[F.trim(F.col(c)).alias(c) if c in str_cols else F.col(c) for c in df.columns]\n",
    "    )\n",
    "\n",
    "def standardize_key(df: DataFrame, col: str, new_col: str) -> DataFrame:\n",
    "    clean_col = F.upper(F.regexp_replace(F.col(col), '[^A-Za-z0-9]', ''))\n",
    "    return df.withColumn(new_col, clean_col)\n",
    "\n",
    "def validate_numeric(df: DataFrame, col: str, min_val: int = None, max_val: int = None) -> DataFrame:\n",
    "    col_val = F.col(col).cast('int')\n",
    "    is_valid = col_val.isNotNull()\n",
    "    if min_val is not None:\n",
    "        is_valid = is_valid & (col_val >= min_val)\n",
    "    if max_val is not None:\n",
    "        is_valid = is_valid & (col_val <= max_val)\n",
    "    return df.filter(is_valid)\n",
    "\n",
    "def validate_date(df: DataFrame, col: str) -> DataFrame:\n",
    "    return df.filter(F.col(col).isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97fa1f0b-8fb3-4445-97dd-6d1ac14732d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from typing import Dict\n",
    "from itertools import chain\n",
    "\n",
    "# ... (Keep your Helper Functions exactly as they are) ...\n",
    "# [validate_numeric, validate_date, trim_all_string_columns, standardize_key]\n",
    "\n",
    "def process_crm_sales_details(bronze_table: str, silver_table: str):\n",
    "    print(f\"Processing {silver_table}...\")\n",
    "    df = spark.table(bronze_table)\n",
    "    \n",
    "    # 1. Clean Strings\n",
    "    df = trim_all_string_columns(df)\n",
    "    \n",
    "    # 2. Standardize Key (Part A: Clean Chars)\n",
    "    df = standardize_key(df, 'sls_prd_key', 'std_sls_prd_key')\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # CRITICAL FIX: Match the Logic from Dim_Product\n",
    "    # ==========================================================================\n",
    "    # We must truncate this to 4 chars so it matches the 'std_prd_key' in the \n",
    "    # Product Dimension (e.g., \"BIKE-123\" -> \"BIKE\").\n",
    "    df = df.withColumn('std_sls_prd_key', F.col('std_sls_prd_key').substr(1, 4))\n",
    "\n",
    "    # 3. Dates & Validation (Your existing logic)\n",
    "    df = df.withColumn('order_date', F.expr(\"try_to_date(CAST(sls_order_dt AS STRING), 'yyyyMMdd')\")) \\\n",
    "           .withColumn('ship_date', F.expr(\"try_to_date(CAST(sls_ship_dt AS STRING), 'yyyyMMdd')\")) \\\n",
    "           .withColumn('due_date', F.expr(\"try_to_date(CAST(sls_due_dt AS STRING), 'yyyyMMdd')\"))\n",
    "    \n",
    "    df = validate_numeric(df, 'sls_sales', min_val=0, max_val=1000000)\n",
    "    df = validate_numeric(df, 'sls_quantity', min_val=0, max_val=10000)\n",
    "    df = validate_numeric(df, 'sls_price', min_val=0, max_val=100000)\n",
    "\n",
    "    # 4. Rename & Select\n",
    "    df = df.withColumnRenamed('sls_ord_num', 'order_number') \\\n",
    "           .withColumnRenamed('sls_prd_key', 'product_key') \\\n",
    "           .withColumnRenamed('sls_cust_id', 'customer_id') \\\n",
    "           .withColumnRenamed('sls_sales', 'sales_amount') \\\n",
    "           .withColumnRenamed('sls_quantity', 'quantity') \\\n",
    "           .withColumnRenamed('sls_price', 'price')\n",
    "\n",
    "    df = df.drop('sls_order_dt', 'sls_ship_dt', 'sls_due_dt')\n",
    "    \n",
    "    # Note: We select 'std_sls_prd_key' to use it for the join later\n",
    "    cols = ['order_number', 'product_key', 'std_sls_prd_key', 'customer_id', 'order_date', 'ship_date', 'due_date', 'sales_amount', 'quantity', 'price']\n",
    "    df = df.select(*cols)\n",
    "    \n",
    "    df.write.mode('overwrite').saveAsTable(silver_table)\n",
    "    display(df)\n",
    "\n",
    "# Run logic\n",
    "process_crm_sales_details('workspace.bronze.crm_sales_details', 'workspace.silver.crm_sales_details')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4095447974261828,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_crm_sales_details",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

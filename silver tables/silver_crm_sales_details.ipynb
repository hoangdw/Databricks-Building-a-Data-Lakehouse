{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08c81881-b3c2-4c45-85f1-cf26f4a1c7e7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View data"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table('workspace.bronze.crm_sales_details')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "809291a0-9bbd-4906-b4b6-7cd2ef0fdb58",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Find duplicates in crm_sales_details"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT sls_ord_num, sls_prd_key, sls_cust_id, sls_order_dt, sls_ship_dt, sls_due_dt, sls_sales, sls_quantity, sls_price, COUNT(*) AS duplicate_count\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "GROUP BY sls_ord_num, sls_prd_key, sls_cust_id, sls_order_dt, sls_ship_dt, sls_due_dt, sls_sales, sls_quantity, sls_price\n",
    "HAVING COUNT(*) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6541cd30-0db4-4e46-811a-5a8efae89456",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check for extra spaces in string columns (SQL)"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT sls_ord_num, sls_prd_key, sls_cust_id, sls_order_dt, sls_ship_dt, sls_due_dt, sls_sales, sls_quantity, sls_price,\n",
    "       CASE WHEN sls_ord_num != TRIM(sls_ord_num) OR LENGTH(sls_ord_num) != LENGTH(TRIM(sls_ord_num)) THEN 'extra_spaces' ELSE '' END AS sls_ord_num_flag,\n",
    "       CASE WHEN sls_prd_key != TRIM(sls_prd_key) OR LENGTH(sls_prd_key) != LENGTH(TRIM(sls_prd_key)) THEN 'extra_spaces' ELSE '' END AS sls_prd_key_flag\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "WHERE (sls_ord_num != TRIM(sls_ord_num) OR LENGTH(sls_ord_num) != LENGTH(TRIM(sls_ord_num)))\n",
    "   OR (sls_prd_key != TRIM(sls_prd_key) OR LENGTH(sls_prd_key) != LENGTH(TRIM(sls_prd_key)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e86d99ea-0b20-4bdc-b34f-b085b236fa5c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validate date values in crm_sales_details (SQL)"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check for missing values in sls_order_dt, sls_ship_dt, sls_due_dt\n",
    "SELECT COUNT(*) AS missing_order_count\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "WHERE sls_order_dt IS NULL;\n",
    "\n",
    "SELECT COUNT(*) AS missing_ship_count\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "WHERE sls_ship_dt IS NULL;\n",
    "\n",
    "SELECT COUNT(*) AS missing_due_count\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "WHERE sls_due_dt IS NULL;\n",
    "\n",
    "-- Show sample date values and their format\n",
    "SELECT DISTINCT sls_order_dt\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "ORDER BY sls_order_dt DESC;\n",
    "\n",
    "SELECT DISTINCT sls_ship_dt\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "ORDER BY sls_ship_dt DESC;\n",
    "\n",
    "SELECT DISTINCT sls_due_dt\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "ORDER BY sls_due_dt DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf0d176-c98d-458e-9692-ca3bd8dce148",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert integer date columns to date type"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.table('workspace.bronze.crm_sales_details')\n",
    "\n",
    "# Convert integer date columns to string, then to date\n",
    "df = df.withColumn('order_date', F.to_date(F.col('sls_order_dt').cast('string'), 'yyyyMMdd')) \\\n",
    "       .withColumn('ship_date', F.to_date(F.col('sls_ship_dt').cast('string'), 'yyyyMMdd')) \\\n",
    "       .withColumn('due_date', F.to_date(F.col('sls_due_dt').cast('string'), 'yyyyMMdd'))\n",
    "\n",
    "# Optionally drop original columns or keep them for traceability\n",
    "df = df.drop('sls_order_dt', 'sls_ship_dt', 'sls_due_dt')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e73adde-a797-4fbe-9935-33953adb2ad9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validate numeric values in crm_sales_details (SQL)"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check for missing values in sls_sales, sls_quantity, sls_price\n",
    "SELECT COUNT(*) AS missing_sales_count\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "WHERE sls_sales IS NULL;\n",
    "\n",
    "SELECT COUNT(*) AS missing_quantity_count\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "WHERE sls_quantity IS NULL;\n",
    "\n",
    "SELECT COUNT(*) AS missing_price_count\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "WHERE sls_price IS NULL;\n",
    "\n",
    "-- Check for outliers in sls_sales, sls_quantity, sls_price\n",
    "SELECT sls_sales\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "WHERE sls_sales < 0 OR sls_sales > 1000000;\n",
    "\n",
    "SELECT sls_quantity\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "WHERE sls_quantity < 0 OR sls_quantity > 10000;\n",
    "\n",
    "SELECT sls_price\n",
    "FROM workspace.bronze.crm_sales_details\n",
    "WHERE sls_price < 0 OR sls_price > 100000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a808a25-0727-4aa7-8570-f2da334b5d66",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Identify issues with product key in crm_sales_details"
    }
   },
   "outputs": [],
   "source": [
    "# List unique values and counts for product key\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "prd_key_counts = spark.table('workspace.bronze.crm_sales_details') \\\n",
    "    .groupBy('sls_prd_key') \\\n",
    "    .count() \\\n",
    "    .orderBy('count', ascending=False)\n",
    "display(prd_key_counts)\n",
    "\n",
    "# Flag unexpected values (e.g., null or empty)\n",
    "unique_keys = [row['sls_prd_key'] for row in prd_key_counts.collect()]\n",
    "issues = [val for val in unique_keys if val is None or val == '']\n",
    "print(\"Unexpected or ambiguous product key values:\", issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4b318d-5797-4ed6-b601-c59978742d4e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Standardize business key IDs in crm_sales_details (SQL)"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Standardize sls_prd_key for joinability\n",
    "SELECT sls_ord_num,\n",
    "       UPPER(TRIM(REGEXP_REPLACE(sls_prd_key, '[^A-Za-z0-9]', ''))) AS std_sls_prd_key,\n",
    "       sls_cust_id,\n",
    "       sls_order_dt,\n",
    "       sls_ship_dt,\n",
    "       sls_due_dt,\n",
    "       sls_sales,\n",
    "       sls_quantity,\n",
    "       sls_price\n",
    "FROM workspace.bronze.crm_sales_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c39c906-4e37-4755-9eed-15dbac4b4d4a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write cleaned and renamed DataFrame to silver table"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read bronze table\n",
    "bronze_df = spark.table('workspace.bronze.crm_sales_details')\n",
    "\n",
    "# Trim all string columns\n",
    "def trim_all_string_columns(df):\n",
    "    for col_name, dtype in df.dtypes:\n",
    "        if dtype == 'string':\n",
    "            df = df.withColumn(col_name, F.trim(F.col(col_name)))\n",
    "    return df\n",
    "\n",
    "bronze_df = trim_all_string_columns(bronze_df)\n",
    "\n",
    "# Convert integer date columns to date type (tolerate invalid values)\n",
    "bronze_df = bronze_df.withColumn('order_date', F.expr(\"try_to_date(CAST(sls_order_dt AS STRING), 'yyyyMMdd')\")) \\\n",
    "    .withColumn('ship_date', F.expr(\"try_to_date(CAST(sls_ship_dt AS STRING), 'yyyyMMdd')\")) \\\n",
    "    .withColumn('due_date', F.expr(\"try_to_date(CAST(sls_due_dt AS STRING), 'yyyyMMdd')\"))\n",
    "\n",
    "# Standardize product key\n",
    "bronze_df = bronze_df.withColumn('std_sls_prd_key', F.upper(F.trim(F.regexp_replace(F.col('sls_prd_key'), '[^A-Za-z0-9]', ''))))\n",
    "\n",
    "# Rename columns to more readable names\n",
    "silver_df = bronze_df.withColumnRenamed('sls_ord_num', 'order_number') \\\n",
    "    .withColumnRenamed('sls_prd_key', 'product_key') \\\n",
    "    .withColumnRenamed('sls_cust_id', 'customer_id') \\\n",
    "    .withColumnRenamed('sls_sales', 'sales_amount') \\\n",
    "    .withColumnRenamed('sls_quantity', 'quantity') \\\n",
    "    .withColumnRenamed('sls_price', 'price')\n",
    "\n",
    "# Drop original integer date columns\n",
    "silver_df = silver_df.drop('sls_order_dt', 'sls_ship_dt', 'sls_due_dt')\n",
    "\n",
    "# Reorder columns for clarity\n",
    "cols = ['order_number', 'product_key', 'std_sls_prd_key', 'customer_id', 'order_date', 'ship_date', 'due_date', 'sales_amount', 'quantity', 'price']\n",
    "silver_df = silver_df.select(*cols)\n",
    "\n",
    "# Write to silver table\n",
    "silver_df.write.mode('overwrite').saveAsTable('workspace.silver.crm_sales_details')\n",
    "display(silver_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4095447974261828,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_crm_sales_details",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

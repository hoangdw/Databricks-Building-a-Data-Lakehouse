{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85ec974c-ad6e-4308-8d96-89a768ecd48d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reusable Data Cleaning Functions"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from typing import Dict, Optional\n",
    "from itertools import chain\n",
    "\n",
    "# ==============================================================================\n",
    "# Helper Functions (Infrastructure)\n",
    "# ==============================================================================\n",
    "\n",
    "def create_map_from_dict(mapping: Dict[str, str]):\n",
    "    \"\"\"Converts a Python Dictionary into a PySpark Map Column.\"\"\"\n",
    "    # --------------------------------------------------------------------------\n",
    "    # WHAT: Convert Python Dict {'k': 'v'} -> Spark Map (k, v).\n",
    "    #       itertools.chain(*mapping.items()) flattens the list of tuples \n",
    "    #       [(k1, v1), (k2, v2)] into [k1, v1, k2, v2].\n",
    "    #       F.create_map expects this flattened list structure.\n",
    "    # WHY:  Decouples Business Logic from Code. It allows you to define complex \n",
    "    #       mappings (like Country Codes) in clean Python dictionaries instead of \n",
    "    #       writing 50 lines of 'F.when().otherwise()' logic.\n",
    "    # --------------------------------------------------------------------------\n",
    "    return F.create_map([F.lit(x) for x in chain(*mapping.items())])\n",
    "\n",
    "# ==============================================================================\n",
    "# Core Transformation Logic\n",
    "# ==============================================================================\n",
    "\n",
    "def trim_all_string_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Trims all string columns in a single projection (O(1) overhead).\"\"\"\n",
    "    # --------------------------------------------------------------------------\n",
    "    # WHAT: Scan the schema to find all columns that are of type 'StringType'.\n",
    "    #       We use a set comprehension for fast O(1) lookups.\n",
    "    # --------------------------------------------------------------------------\n",
    "    str_cols = {f.name for f in df.schema.fields if isinstance(f.dataType, StringType)}\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # WHAT: Reconstruct the DataFrame in ONE 'select' statement.\n",
    "    #       - If col is a string: Apply F.trim().\n",
    "    #       - If col is not a string: Pass it through (F.col()).\n",
    "    # WHY:  Performance (DAG Optimization). \n",
    "    #       A naive approach uses a 'for' loop with 'df.withColumn()'. \n",
    "    #       If you have 100 columns, that creates 100 separate plans (DAG Explosion).\n",
    "    #       This approach creates 1 plan. It is 10x-50x faster on wide tables.\n",
    "    # --------------------------------------------------------------------------\n",
    "    return df.select(\n",
    "        *[F.trim(F.col(c)).alias(c) if c in str_cols else F.col(c) for c in df.columns]\n",
    "    )\n",
    "\n",
    "def standardize_key(df: DataFrame, col: str, new_col: str) -> DataFrame:\n",
    "    \"\"\"Standardizes a key by uppercasing and removing non-alphanumeric chars.\"\"\"\n",
    "    # --------------------------------------------------------------------------\n",
    "    # WHAT: Apply Regex Replacement '[^A-Za-z0-9]'.\n",
    "    #       This removes anything that is NOT a letter or number (e.g., spaces, -, _).\n",
    "    #       Then apply F.upper() to normalize case.\n",
    "    # WHY:  Join Reliability. \n",
    "    #       \"Product-A \" and \"product_a\" should be treated as the exact same key.\n",
    "    #       This creates a robust Join Key for Silver->Gold transformations.\n",
    "    # --------------------------------------------------------------------------\n",
    "    clean_col = F.upper(F.regexp_replace(F.col(col), '[^A-Za-z0-9]', ''))\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # WHAT: Add the new standardized key as a new column.\n",
    "    # WHY:  Non-destructive. We keep the original 'col' for audit/debug purposes\n",
    "    #       but use 'new_col' for actual joining.\n",
    "    # --------------------------------------------------------------------------\n",
    "    return df.withColumn(new_col, clean_col)\n",
    "\n",
    "def normalize_categorical(df: DataFrame, col: str, rules: Dict[str, str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Generic function to normalize any categorical column using a dictionary.\n",
    "    \"\"\"\n",
    "    # --------------------------------------------------------------------------\n",
    "    # WHAT: Convert the Python rules dictionary into a Spark Map column.\n",
    "    # --------------------------------------------------------------------------\n",
    "    map_col = create_map_from_dict(rules)\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # WHAT: Perform a Key-Value lookup.\n",
    "    #       map_col[F.upper(F.col(col))] attempts to find the value in the map.\n",
    "    #       F.coalesce(..., F.col(col)) provides the \"Else\" logic.\n",
    "    #       If the lookup returns NULL (key not found), keep the original value.\n",
    "    # WHY:  Safe Standardization. \n",
    "    #       If we receive a new code 'Z' that isn't in our dictionary, we don't \n",
    "    #       want to null it out. We pass 'Z' through so we can see it and update \n",
    "    #       our dictionary later.\n",
    "    # --------------------------------------------------------------------------\n",
    "    return df.withColumn(col, F.coalesce(map_col[F.upper(F.col(col))], F.col(col)))\n",
    "\n",
    "def flag_invalid_numeric(df: DataFrame, col: str, min_val: Optional[int] = None, max_val: Optional[int] = None) -> DataFrame:\n",
    "    \"\"\"Flags rows that are not valid integers within range.\"\"\"\n",
    "    # --------------------------------------------------------------------------\n",
    "    # WHAT: Safe Casting. Try to turn the string into an integer.\n",
    "    #       Spark returns NULL if the cast fails (e.g., cast(\"Apple\" as int) -> NULL).\n",
    "    # --------------------------------------------------------------------------\n",
    "    col_val = F.col(col).cast('int')\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # WHAT: The Base Condition. It must be successfully castable (Not Null).\n",
    "    # --------------------------------------------------------------------------\n",
    "    is_valid = col_val.isNotNull()\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # WHAT: Conditional Range Checks.\n",
    "    #       If the user provided min/max constraints, add them to the boolean logic.\n",
    "    #       We use '&' (Bitwise AND) to chain boolean conditions in Spark.\n",
    "    # --------------------------------------------------------------------------\n",
    "    if min_val is not None:\n",
    "        is_valid = is_valid & (col_val >= min_val)\n",
    "    if max_val is not None:\n",
    "        is_valid = is_valid & (col_val <= max_val)\n",
    "        \n",
    "    # --------------------------------------------------------------------------\n",
    "    # WHAT: Return the DataFrame with a NEW boolean column (True/False).\n",
    "    # WHY:  \"Soft Failure\" Strategy. Instead of dropping bad rows (data loss), \n",
    "    #       we tag them. This allows the Gold layer to decide: \n",
    "    #       \"Do I exclude them?\" or \"Do I include them for a Data Quality Report?\"\n",
    "    # --------------------------------------------------------------------------\n",
    "    return df.withColumn(f\"is_valid_{col}\", is_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "449cd2eb-87fb-4212-b126-2626a2abc641",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean and Write Silver Table"
    }
   },
   "outputs": [],
   "source": [
    "def process_crm_cust_info(bronze_table: str, silver_table: str):\n",
    "    print(f\"Starting processing for {silver_table}...\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # STEP 0: DEFINE BUSINESS RULES (Configuration)\n",
    "    # ==========================================================================\n",
    "    # WHAT: Define dictionaries mapping raw codes to clean, human-readable values.\n",
    "    # WHY:  Centralized Logic. Instead of burying these rules inside a complex SQL \n",
    "    #       'CASE WHEN' statement, we define them here. This handles dirty data \n",
    "    #       variations (e.g., 'F' and 'FEMALE' map to the same target).\n",
    "    gender_rules = {\n",
    "        'F': 'Female', 'M': 'Male', \n",
    "        'FEMALE': 'Female', 'MALE': 'Male', 'UNK': 'Unknown'\n",
    "    }\n",
    "    marital_rules = {\n",
    "        'S': 'Single', 'M': 'Married',\n",
    "        'SINGLE': 'Single', 'MARRIED': 'Married'\n",
    "    }\n",
    "\n",
    "    # ==========================================================================\n",
    "    # STEP 1: READ (Ingestion)\n",
    "    # ==========================================================================\n",
    "    # WHAT: Lazy load the Bronze table into a DataFrame.\n",
    "    # WHY:  'spark.table' connects to the Unity Catalog metastore, preserving \n",
    "    #       lineage and access controls defined in the platform.\n",
    "    df_bronze = spark.table(bronze_table)\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # STEP 2: TRANSFORM PIPELINE (Cleaning)\n",
    "    # ==========================================================================\n",
    "    # WHAT: Chain operations.\n",
    "    #       1. trim_all_string_columns: Removes leading/trailing whitespace.\n",
    "    #       2. standardize_key: cleans 'cst_key' -> 'customer_key' (Upper + Regex).\n",
    "    #       3. normalize_categorical: Applies the 'gender_rules' dict to 'cst_gndr'.\n",
    "    #       4. normalize_categorical: Applies 'marital_rules' to 'cst_marital_status'.\n",
    "    #       5. flag_invalid_numeric: Checks if 'cst_id' is a valid integer.\n",
    "    # WHY:  Standardization. We ensure \" Male \" becomes \"Male\" (trim) and then \n",
    "    #       apply the dictionary lookup. This prepares the data for joining.\n",
    "    df_silver = (df_bronze\n",
    "        .transform(trim_all_string_columns)\n",
    "        .transform(lambda df: standardize_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f155bfd-468a-4673-89a3-9129ad36fcb2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table('workspace.silver.crm_cust_info'))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4753146017053377,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_crm_cust_info",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

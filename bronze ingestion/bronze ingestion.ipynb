{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe46360-d94d-4a2a-a299-5521e55f5bae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read and Write CRM DataFrames to Bronze tables"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 1: DEFINE FILE PATHS (Configuration)\n",
    "# ==============================================================================\n",
    "# WHAT: Define string variables pointing to the raw CSV files in the 'Volumes'.\n",
    "# WHY:  Separating configuration (paths) from logic (reading) makes the code cleaner.\n",
    "#       'Volumes' in Unity Catalog are the standard landing zone for raw files.\n",
    "cust_info_path = '/Volumes/workspace/bronze/raw_resources/crm/cust_info.csv'\n",
    "prd_info_path = '/Volumes/workspace/bronze/raw_resources/crm/prd_info.csv'\n",
    "sales_details_path = '/Volumes/workspace/bronze/raw_resources/crm/sales_details.csv'\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: READ RAW DATA (Ingestion)\n",
    "# ==============================================================================\n",
    "# WHAT: Read the CSVs into Spark DataFrames.\n",
    "#       header=True: Uses the first row of the CSV as column names.\n",
    "#       inferSchema=True: Spark scans the file to guess if columns are Integer, String, etc.\n",
    "# WHY:  This converts unstructured text files into structured DataFrames we can query.\n",
    "#       Note: 'inferSchema' is expensive for huge files but perfect for initial raw loading.\n",
    "cust_info_df = spark.read.csv(cust_info_path, header=True, inferSchema=True)\n",
    "prd_info_df = spark.read.csv(prd_info_path, header=True, inferSchema=True)\n",
    "sales_details_df = spark.read.csv(sales_details_path, header=True, inferSchema=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 3: VISUALIZE (Sanity Check)\n",
    "# ==============================================================================\n",
    "# WHAT: Trigger an action to show the first 1000 rows of the DataFrames in the UI.\n",
    "# WHY:  Allows the engineer to visually confirm the data loaded correctly \n",
    "#       (e.g., checking if headers are aligned and characters aren't garbled).\n",
    "display(cust_info_df)\n",
    "display(prd_info_df)\n",
    "display(sales_details_df)\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 4: WRITE TO BRONZE (Persistence)\n",
    "# ==============================================================================\n",
    "# WHAT: Write the DataFrames to permanent tables in the Unity Catalog ('workspace.bronze').\n",
    "#       mode('overwrite'): If the table exists, replace it entirely.\n",
    "# WHY:  1. 'saveAsTable' registers the data in the Metastore so you can query it with SQL later.\n",
    "#       2. 'overwrite' ensures Idempotency: You can run this notebook 10 times, \n",
    "#          and the result is always the same (no duplicate rows appended).\n",
    "#       3. 'crm_' prefix: Namespaces the tables so we know they came from the CRM system.\n",
    "cust_info_df.write.mode('overwrite').saveAsTable('workspace.bronze.crm_cust_info')\n",
    "prd_info_df.write.mode('overwrite').saveAsTable('workspace.bronze.crm_prd_info')\n",
    "sales_details_df.write.mode('overwrite').saveAsTable('workspace.bronze.crm_sales_details')\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 5: VERIFY (Post-Write Check)\n",
    "# ==============================================================================\n",
    "# WHAT: Read the *newly created tables* back from the database and display them.\n",
    "# WHY:  This confirms the write operation was successful and the data is strictly \n",
    "#       available in the Bronze layer for downstream (Silver) processing.\n",
    "display(spark.table('workspace.bronze.crm_cust_info'))\n",
    "display(spark.table('workspace.bronze.crm_prd_info'))\n",
    "display(spark.table('workspace.bronze.crm_sales_details'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b85933-3a86-4c84-9d99-b3bfb8aff394",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771082356623}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Write CRM DataFrames to Bronze tables"
    }
   },
   "outputs": [],
   "source": [
    "# Read each ERP CSV file separately into its own DataFrame\n",
    "erp_cust_az12_path = '/Volumes/workspace/bronze/raw_resources/erp/CUST_AZ12.csv'\n",
    "erp_loc_a101_path = '/Volumes/workspace/bronze/raw_resources/erp/LOC_A101.csv'\n",
    "erp_px_cat_g1v2_path = '/Volumes/workspace/bronze/raw_resources/erp/PX_CAT_G1V2.csv'\n",
    "\n",
    "erp_cust_az12_df = spark.read.csv(erp_cust_az12_path, header=True, inferSchema=True)\n",
    "erp_loc_a101_df = spark.read.csv(erp_loc_a101_path, header=True, inferSchema=True)\n",
    "erp_px_cat_g1v2_df = spark.read.csv(erp_px_cat_g1v2_path, header=True, inferSchema=True)\n",
    "\n",
    "display(erp_cust_az12_df)\n",
    "display(erp_loc_a101_df)\n",
    "display(erp_px_cat_g1v2_df)\n",
    "\n",
    "# Write each ERP DataFrame to a Unity Catalog table in Bronze schema with source-system prefix\n",
    "erp_cust_az12_df.write.mode('overwrite').saveAsTable('workspace.bronze.erp_cust_az12')\n",
    "erp_loc_a101_df.write.mode('overwrite').saveAsTable('workspace.bronze.erp_loc_a101')\n",
    "erp_px_cat_g1v2_df.write.mode('overwrite').saveAsTable('workspace.bronze.erp_px_cat_g1v2')\n",
    "\n",
    "# Verify writes by reading the tables\n",
    "display(spark.table('workspace.bronze.erp_cust_az12'))\n",
    "display(spark.table('workspace.bronze.erp_loc_a101'))\n",
    "display(spark.table('workspace.bronze.erp_px_cat_g1v2'))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
